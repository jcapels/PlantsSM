{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcapela/PlantsSM/src/plants_sm/io/csv.py:56: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(self.path, **self.kwargs)\n"
     ]
    }
   ],
   "source": [
    "from plants_sm.data_structures.dataset.single_input_dataset import SingleInputDataset\n",
    "\n",
    "dataset = SingleInputDataset.from_csv(\"../../final_data/test_300.csv\", representation_field=\"sequence\", instances_ids_field=\"accession\", labels_field=slice(8, 2779))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncator: 100%|██████████| 300/300 [00:00<00:00, 42826.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from plants_sm.data_standardization.truncation import Truncator\n",
    "dataset = Truncator(max_length=800).fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from plants_sm.models.ec_number_prediction.esm import EC_ESM_Lightning\n",
    "\n",
    "model = EC_ESM_Lightning(\"esm2_t12_35M_UR50D\", [2560, 5120], 2771)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'esm_model.layers.0.fc1.bias',\n",
       " 'esm_model.layers.0.fc1.weight',\n",
       " 'esm_model.layers.0.fc2.bias',\n",
       " 'esm_model.layers.0.fc2.weight',\n",
       " 'esm_model.layers.0.final_layer_norm.bias',\n",
       " 'esm_model.layers.0.final_layer_norm.weight',\n",
       " 'esm_model.layers.0.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.0.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.0.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.0.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.0.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.0.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.0.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.0.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.0.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.0.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.1.fc1.bias',\n",
       " 'esm_model.layers.1.fc1.weight',\n",
       " 'esm_model.layers.1.fc2.bias',\n",
       " 'esm_model.layers.1.fc2.weight',\n",
       " 'esm_model.layers.1.final_layer_norm.bias',\n",
       " 'esm_model.layers.1.final_layer_norm.weight',\n",
       " 'esm_model.layers.1.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.1.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.1.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.1.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.1.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.1.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.1.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.1.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.1.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.1.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.2.fc1.bias',\n",
       " 'esm_model.layers.2.fc1.weight',\n",
       " 'esm_model.layers.2.fc2.bias',\n",
       " 'esm_model.layers.2.fc2.weight',\n",
       " 'esm_model.layers.2.final_layer_norm.bias',\n",
       " 'esm_model.layers.2.final_layer_norm.weight',\n",
       " 'esm_model.layers.2.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.2.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.2.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.2.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.2.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.2.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.2.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.2.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.2.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.2.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.3.fc1.bias',\n",
       " 'esm_model.layers.3.fc1.weight',\n",
       " 'esm_model.layers.3.fc2.bias',\n",
       " 'esm_model.layers.3.fc2.weight',\n",
       " 'esm_model.layers.3.final_layer_norm.bias',\n",
       " 'esm_model.layers.3.final_layer_norm.weight',\n",
       " 'esm_model.layers.3.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.3.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.3.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.3.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.3.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.3.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.3.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.3.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.3.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.3.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.4.fc1.bias',\n",
       " 'esm_model.layers.4.fc1.weight',\n",
       " 'esm_model.layers.4.fc2.bias',\n",
       " 'esm_model.layers.4.fc2.weight',\n",
       " 'esm_model.layers.4.final_layer_norm.bias',\n",
       " 'esm_model.layers.4.final_layer_norm.weight',\n",
       " 'esm_model.layers.4.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.4.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.4.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.4.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.4.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.4.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.4.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.4.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.4.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.4.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.5.fc1.bias',\n",
       " 'esm_model.layers.5.fc1.weight',\n",
       " 'esm_model.layers.5.fc2.bias',\n",
       " 'esm_model.layers.5.fc2.weight',\n",
       " 'esm_model.layers.5.final_layer_norm.bias',\n",
       " 'esm_model.layers.5.final_layer_norm.weight',\n",
       " 'esm_model.layers.5.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.5.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.5.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.5.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.5.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.5.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.5.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.5.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.5.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.5.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.6.fc1.bias',\n",
       " 'esm_model.layers.6.fc1.weight',\n",
       " 'esm_model.layers.6.fc2.bias',\n",
       " 'esm_model.layers.6.fc2.weight',\n",
       " 'esm_model.layers.6.final_layer_norm.bias',\n",
       " 'esm_model.layers.6.final_layer_norm.weight',\n",
       " 'esm_model.layers.6.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.6.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.6.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.6.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.6.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.6.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.6.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.6.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.6.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.6.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.7.fc1.bias',\n",
       " 'esm_model.layers.7.fc1.weight',\n",
       " 'esm_model.layers.7.fc2.bias',\n",
       " 'esm_model.layers.7.fc2.weight',\n",
       " 'esm_model.layers.7.final_layer_norm.bias',\n",
       " 'esm_model.layers.7.final_layer_norm.weight',\n",
       " 'esm_model.layers.7.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.7.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.7.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.7.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.7.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.7.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.7.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.7.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.7.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.7.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.8.fc1.bias',\n",
       " 'esm_model.layers.8.fc1.weight',\n",
       " 'esm_model.layers.8.fc2.bias',\n",
       " 'esm_model.layers.8.fc2.weight',\n",
       " 'esm_model.layers.8.final_layer_norm.bias',\n",
       " 'esm_model.layers.8.final_layer_norm.weight',\n",
       " 'esm_model.layers.8.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.8.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.8.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.8.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.8.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.8.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.8.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.8.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.8.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.8.self_attn_layer_norm.weight',\n",
       " 'esm_model.layers.9.fc1.bias',\n",
       " 'esm_model.layers.9.fc1.weight',\n",
       " 'esm_model.layers.9.fc2.bias',\n",
       " 'esm_model.layers.9.fc2.weight',\n",
       " 'esm_model.layers.9.final_layer_norm.bias',\n",
       " 'esm_model.layers.9.final_layer_norm.weight',\n",
       " 'esm_model.layers.9.self_attn.k_proj.bias',\n",
       " 'esm_model.layers.9.self_attn.k_proj.weight',\n",
       " 'esm_model.layers.9.self_attn.out_proj.bias',\n",
       " 'esm_model.layers.9.self_attn.out_proj.weight',\n",
       " 'esm_model.layers.9.self_attn.q_proj.bias',\n",
       " 'esm_model.layers.9.self_attn.q_proj.weight',\n",
       " 'esm_model.layers.9.self_attn.v_proj.bias',\n",
       " 'esm_model.layers.9.self_attn.v_proj.weight',\n",
       " 'esm_model.layers.9.self_attn_layer_norm.bias',\n",
       " 'esm_model.layers.9.self_attn_layer_norm.weight'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plants_sm.featurization.proteins.bio_embeddings.constants import ESM_LAYERS\n",
    "\n",
    "\n",
    "layers_1 = ESM_LAYERS[\"esm2_t12_35M_UR50D\"] - 1\n",
    "layer_2 = ESM_LAYERS[\"esm2_t12_35M_UR50D\"] - 2\n",
    "no_grad = set()\n",
    "for parameter in model.named_parameters():\n",
    "    if \"layers\" in parameter[0] and str(layers_1) not in parameter[0] and str(layer_2) not in parameter[0]:\n",
    "        no_grad.add(parameter[0])\n",
    "no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm_model.embed_tokens.weight\n",
      "esm_model.layers.0.self_attn.k_proj.weight\n",
      "esm_model.layers.0.self_attn.k_proj.bias\n",
      "esm_model.layers.0.self_attn.v_proj.weight\n",
      "esm_model.layers.0.self_attn.v_proj.bias\n",
      "esm_model.layers.0.self_attn.q_proj.weight\n",
      "esm_model.layers.0.self_attn.q_proj.bias\n",
      "esm_model.layers.0.self_attn.out_proj.weight\n",
      "esm_model.layers.0.self_attn.out_proj.bias\n",
      "esm_model.layers.0.self_attn_layer_norm.weight\n",
      "esm_model.layers.0.self_attn_layer_norm.bias\n",
      "esm_model.layers.0.fc1.weight\n",
      "esm_model.layers.0.fc1.bias\n",
      "esm_model.layers.0.fc2.weight\n",
      "esm_model.layers.0.fc2.bias\n",
      "esm_model.layers.0.final_layer_norm.weight\n",
      "esm_model.layers.0.final_layer_norm.bias\n",
      "esm_model.layers.1.self_attn.k_proj.weight\n",
      "esm_model.layers.1.self_attn.k_proj.bias\n",
      "esm_model.layers.1.self_attn.v_proj.weight\n",
      "esm_model.layers.1.self_attn.v_proj.bias\n",
      "esm_model.layers.1.self_attn.q_proj.weight\n",
      "esm_model.layers.1.self_attn.q_proj.bias\n",
      "esm_model.layers.1.self_attn.out_proj.weight\n",
      "esm_model.layers.1.self_attn.out_proj.bias\n",
      "esm_model.layers.1.self_attn_layer_norm.weight\n",
      "esm_model.layers.1.self_attn_layer_norm.bias\n",
      "esm_model.layers.1.fc1.weight\n",
      "esm_model.layers.1.fc1.bias\n",
      "esm_model.layers.1.fc2.weight\n",
      "esm_model.layers.1.fc2.bias\n",
      "esm_model.layers.1.final_layer_norm.weight\n",
      "esm_model.layers.1.final_layer_norm.bias\n",
      "esm_model.layers.2.self_attn.k_proj.weight\n",
      "esm_model.layers.2.self_attn.k_proj.bias\n",
      "esm_model.layers.2.self_attn.v_proj.weight\n",
      "esm_model.layers.2.self_attn.v_proj.bias\n",
      "esm_model.layers.2.self_attn.q_proj.weight\n",
      "esm_model.layers.2.self_attn.q_proj.bias\n",
      "esm_model.layers.2.self_attn.out_proj.weight\n",
      "esm_model.layers.2.self_attn.out_proj.bias\n",
      "esm_model.layers.2.self_attn_layer_norm.weight\n",
      "esm_model.layers.2.self_attn_layer_norm.bias\n",
      "esm_model.layers.2.fc1.weight\n",
      "esm_model.layers.2.fc1.bias\n",
      "esm_model.layers.2.fc2.weight\n",
      "esm_model.layers.2.fc2.bias\n",
      "esm_model.layers.2.final_layer_norm.weight\n",
      "esm_model.layers.2.final_layer_norm.bias\n",
      "esm_model.layers.3.self_attn.k_proj.weight\n",
      "esm_model.layers.3.self_attn.k_proj.bias\n",
      "esm_model.layers.3.self_attn.v_proj.weight\n",
      "esm_model.layers.3.self_attn.v_proj.bias\n",
      "esm_model.layers.3.self_attn.q_proj.weight\n",
      "esm_model.layers.3.self_attn.q_proj.bias\n",
      "esm_model.layers.3.self_attn.out_proj.weight\n",
      "esm_model.layers.3.self_attn.out_proj.bias\n",
      "esm_model.layers.3.self_attn_layer_norm.weight\n",
      "esm_model.layers.3.self_attn_layer_norm.bias\n",
      "esm_model.layers.3.fc1.weight\n",
      "esm_model.layers.3.fc1.bias\n",
      "esm_model.layers.3.fc2.weight\n",
      "esm_model.layers.3.fc2.bias\n",
      "esm_model.layers.3.final_layer_norm.weight\n",
      "esm_model.layers.3.final_layer_norm.bias\n",
      "esm_model.layers.4.self_attn.k_proj.weight\n",
      "esm_model.layers.4.self_attn.k_proj.bias\n",
      "esm_model.layers.4.self_attn.v_proj.weight\n",
      "esm_model.layers.4.self_attn.v_proj.bias\n",
      "esm_model.layers.4.self_attn.q_proj.weight\n",
      "esm_model.layers.4.self_attn.q_proj.bias\n",
      "esm_model.layers.4.self_attn.out_proj.weight\n",
      "esm_model.layers.4.self_attn.out_proj.bias\n",
      "esm_model.layers.4.self_attn_layer_norm.weight\n",
      "esm_model.layers.4.self_attn_layer_norm.bias\n",
      "esm_model.layers.4.fc1.weight\n",
      "esm_model.layers.4.fc1.bias\n",
      "esm_model.layers.4.fc2.weight\n",
      "esm_model.layers.4.fc2.bias\n",
      "esm_model.layers.4.final_layer_norm.weight\n",
      "esm_model.layers.4.final_layer_norm.bias\n",
      "esm_model.layers.5.self_attn.k_proj.weight\n",
      "esm_model.layers.5.self_attn.k_proj.bias\n",
      "esm_model.layers.5.self_attn.v_proj.weight\n",
      "esm_model.layers.5.self_attn.v_proj.bias\n",
      "esm_model.layers.5.self_attn.q_proj.weight\n",
      "esm_model.layers.5.self_attn.q_proj.bias\n",
      "esm_model.layers.5.self_attn.out_proj.weight\n",
      "esm_model.layers.5.self_attn.out_proj.bias\n",
      "esm_model.layers.5.self_attn_layer_norm.weight\n",
      "esm_model.layers.5.self_attn_layer_norm.bias\n",
      "esm_model.layers.5.fc1.weight\n",
      "esm_model.layers.5.fc1.bias\n",
      "esm_model.layers.5.fc2.weight\n",
      "esm_model.layers.5.fc2.bias\n",
      "esm_model.layers.5.final_layer_norm.weight\n",
      "esm_model.layers.5.final_layer_norm.bias\n",
      "esm_model.layers.6.self_attn.k_proj.weight\n",
      "esm_model.layers.6.self_attn.k_proj.bias\n",
      "esm_model.layers.6.self_attn.v_proj.weight\n",
      "esm_model.layers.6.self_attn.v_proj.bias\n",
      "esm_model.layers.6.self_attn.q_proj.weight\n",
      "esm_model.layers.6.self_attn.q_proj.bias\n",
      "esm_model.layers.6.self_attn.out_proj.weight\n",
      "esm_model.layers.6.self_attn.out_proj.bias\n",
      "esm_model.layers.6.self_attn_layer_norm.weight\n",
      "esm_model.layers.6.self_attn_layer_norm.bias\n",
      "esm_model.layers.6.fc1.weight\n",
      "esm_model.layers.6.fc1.bias\n",
      "esm_model.layers.6.fc2.weight\n",
      "esm_model.layers.6.fc2.bias\n",
      "esm_model.layers.6.final_layer_norm.weight\n",
      "esm_model.layers.6.final_layer_norm.bias\n",
      "esm_model.layers.7.self_attn.k_proj.weight\n",
      "esm_model.layers.7.self_attn.k_proj.bias\n",
      "esm_model.layers.7.self_attn.v_proj.weight\n",
      "esm_model.layers.7.self_attn.v_proj.bias\n",
      "esm_model.layers.7.self_attn.q_proj.weight\n",
      "esm_model.layers.7.self_attn.q_proj.bias\n",
      "esm_model.layers.7.self_attn.out_proj.weight\n",
      "esm_model.layers.7.self_attn.out_proj.bias\n",
      "esm_model.layers.7.self_attn_layer_norm.weight\n",
      "esm_model.layers.7.self_attn_layer_norm.bias\n",
      "esm_model.layers.7.fc1.weight\n",
      "esm_model.layers.7.fc1.bias\n",
      "esm_model.layers.7.fc2.weight\n",
      "esm_model.layers.7.fc2.bias\n",
      "esm_model.layers.7.final_layer_norm.weight\n",
      "esm_model.layers.7.final_layer_norm.bias\n",
      "esm_model.layers.8.self_attn.k_proj.weight\n",
      "esm_model.layers.8.self_attn.k_proj.bias\n",
      "esm_model.layers.8.self_attn.v_proj.weight\n",
      "esm_model.layers.8.self_attn.v_proj.bias\n",
      "esm_model.layers.8.self_attn.q_proj.weight\n",
      "esm_model.layers.8.self_attn.q_proj.bias\n",
      "esm_model.layers.8.self_attn.out_proj.weight\n",
      "esm_model.layers.8.self_attn.out_proj.bias\n",
      "esm_model.layers.8.self_attn_layer_norm.weight\n",
      "esm_model.layers.8.self_attn_layer_norm.bias\n",
      "esm_model.layers.8.fc1.weight\n",
      "esm_model.layers.8.fc1.bias\n",
      "esm_model.layers.8.fc2.weight\n",
      "esm_model.layers.8.fc2.bias\n",
      "esm_model.layers.8.final_layer_norm.weight\n",
      "esm_model.layers.8.final_layer_norm.bias\n",
      "esm_model.layers.9.self_attn.k_proj.weight\n",
      "esm_model.layers.9.self_attn.k_proj.bias\n",
      "esm_model.layers.9.self_attn.v_proj.weight\n",
      "esm_model.layers.9.self_attn.v_proj.bias\n",
      "esm_model.layers.9.self_attn.q_proj.weight\n",
      "esm_model.layers.9.self_attn.q_proj.bias\n",
      "esm_model.layers.9.self_attn.out_proj.weight\n",
      "esm_model.layers.9.self_attn.out_proj.bias\n",
      "esm_model.layers.9.self_attn_layer_norm.weight\n",
      "esm_model.layers.9.self_attn_layer_norm.bias\n",
      "esm_model.layers.9.fc1.weight\n",
      "esm_model.layers.9.fc1.bias\n",
      "esm_model.layers.9.fc2.weight\n",
      "esm_model.layers.9.fc2.bias\n",
      "esm_model.layers.9.final_layer_norm.weight\n",
      "esm_model.layers.9.final_layer_norm.bias\n",
      "esm_model.layers.10.self_attn.k_proj.weight\n",
      "esm_model.layers.10.self_attn.k_proj.bias\n",
      "esm_model.layers.10.self_attn.v_proj.weight\n",
      "esm_model.layers.10.self_attn.v_proj.bias\n",
      "esm_model.layers.10.self_attn.q_proj.weight\n",
      "esm_model.layers.10.self_attn.q_proj.bias\n",
      "esm_model.layers.10.self_attn.out_proj.weight\n",
      "esm_model.layers.10.self_attn.out_proj.bias\n",
      "esm_model.layers.10.self_attn_layer_norm.weight\n",
      "esm_model.layers.10.self_attn_layer_norm.bias\n",
      "esm_model.layers.10.fc1.weight\n",
      "esm_model.layers.10.fc1.bias\n",
      "esm_model.layers.10.fc2.weight\n",
      "esm_model.layers.10.fc2.bias\n",
      "esm_model.layers.10.final_layer_norm.weight\n",
      "esm_model.layers.10.final_layer_norm.bias\n",
      "esm_model.layers.11.self_attn.k_proj.weight\n",
      "esm_model.layers.11.self_attn.k_proj.bias\n",
      "esm_model.layers.11.self_attn.v_proj.weight\n",
      "esm_model.layers.11.self_attn.v_proj.bias\n",
      "esm_model.layers.11.self_attn.q_proj.weight\n",
      "esm_model.layers.11.self_attn.q_proj.bias\n",
      "esm_model.layers.11.self_attn.out_proj.weight\n",
      "esm_model.layers.11.self_attn.out_proj.bias\n",
      "esm_model.layers.11.self_attn_layer_norm.weight\n",
      "esm_model.layers.11.self_attn_layer_norm.bias\n",
      "esm_model.layers.11.fc1.weight\n",
      "esm_model.layers.11.fc1.bias\n",
      "esm_model.layers.11.fc2.weight\n",
      "esm_model.layers.11.fc2.bias\n",
      "esm_model.layers.11.final_layer_norm.weight\n",
      "esm_model.layers.11.final_layer_norm.bias\n",
      "esm_model.contact_head.regression.weight\n",
      "esm_model.contact_head.regression.bias\n",
      "esm_model.emb_layer_norm_after.weight\n",
      "esm_model.emb_layer_norm_after.bias\n",
      "esm_model.lm_head.bias\n",
      "esm_model.lm_head.dense.weight\n",
      "esm_model.lm_head.dense.bias\n",
      "esm_model.lm_head.layer_norm.weight\n",
      "esm_model.lm_head.layer_norm.bias\n",
      "dnn.fc_initial.weight\n",
      "dnn.fc_initial.bias\n",
      "dnn.batch_norm_initial.weight\n",
      "dnn.batch_norm_initial.bias\n",
      "dnn.fc1.weight\n",
      "dnn.fc1.bias\n",
      "dnn.batch_norm_layer1.weight\n",
      "dnn.batch_norm_layer1.bias\n",
      "dnn.fc_final.weight\n",
      "dnn.fc_final.bias\n",
      "dnn.final_batch_norm.weight\n",
      "dnn.final_batch_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.named_parameters():\n",
    "    print(parameter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from plants_sm.models.language_pytorch_models import EC_ESM1bLightningModel\n",
    "\n",
    "model_l = EC_ESM1bLightningModel(model, max_epochs=2, devices=[1, 2], batch_size=2, strategy=\"ddp_notebook\",\n",
    "                               accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model_l.preprocess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f73300d3340>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/jcapela/PlantsSM/src/plants_sm/parallelisation/torch_spawner.py\", line 127, in distributed_worker\n    results = main_func(**kwargs)\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n    call._call_and_handle_interrupt(\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 107, in launch\n    _check_bad_cuda_fork()\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/fabric/strategies/launchers/multiprocessing.py\", line 205, in _check_bad_cuda_fork\n    raise RuntimeError(message)\nRuntimeError: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/jcapela/PlantsSM/examples/test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jcapela/PlantsSM/examples/test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_l\u001b[39m.\u001b[39;49mfit(dataset)\n",
      "File \u001b[0;32m~/PlantsSM/src/plants_sm/models/model.py:155\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, train_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, train_dataset: Dataset, validation_dataset: Dataset \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    Fits the model to the data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m    self\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_data(train_dataset, validation_dataset)\n",
      "File \u001b[0;32m~/PlantsSM/src/plants_sm/models/lightning_model.py:95\u001b[0m, in \u001b[0;36mLightningModel._fit_data\u001b[0;34m(self, train_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m     92\u001b[0m     validation_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(validation_dataset, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddp:\n\u001b[0;32m---> 95\u001b[0m     TorchSpawner()\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit, model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataset_loader, val_dataloaders\u001b[39m=\u001b[39;49mvalidation_dataset)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfit(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, train_dataloaders\u001b[39m=\u001b[39mtrain_dataset_loader)\n",
      "File \u001b[0;32m~/PlantsSM/src/plants_sm/parallelisation/torch_spawner.py:57\u001b[0m, in \u001b[0;36mTorchSpawner.run\u001b[0;34m(self, main_func, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m world_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     55\u001b[0m results_file \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mNamedTemporaryFile(delete\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m mp\u001b[39m.\u001b[39;49mspawn(\n\u001b[1;32m     58\u001b[0m     TorchSpawner\u001b[39m.\u001b[39;49mdistributed_worker,\n\u001b[1;32m     59\u001b[0m     nprocs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     60\u001b[0m     args\u001b[39m=\u001b[39;49m(\n\u001b[1;32m     61\u001b[0m         main_func,\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend,\n\u001b[1;32m     63\u001b[0m         world_size,\n\u001b[1;32m     64\u001b[0m         \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmachine_rank,\n\u001b[1;32m     66\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_url,\n\u001b[1;32m     67\u001b[0m         results_file\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m     68\u001b[0m         kwargs\n\u001b[1;32m     69\u001b[0m     ),\n\u001b[1;32m     70\u001b[0m     daemon\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m results \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(results_file)\n\u001b[1;32m     74\u001b[0m results_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/plants_sm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/plants_sm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/plants_sm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/jcapela/PlantsSM/src/plants_sm/parallelisation/torch_spawner.py\", line 127, in distributed_worker\n    results = main_func(**kwargs)\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n    call._call_and_handle_interrupt(\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 107, in launch\n    _check_bad_cuda_fork()\n  File \"/home/jcapela/miniforge3/envs/plants_sm/lib/python3.9/site-packages/lightning/fabric/strategies/launchers/multiprocessing.py\", line 205, in _check_bad_cuda_fork\n    raise RuntimeError(message)\nRuntimeError: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy.\n"
     ]
    }
   ],
   "source": [
    "model_l.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plants_sm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
